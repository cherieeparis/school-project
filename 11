import numpy as np
import scipy as sp
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
# import dataset
df = pd.read_csv("https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Caschool.csv")
df.head(5)

# pick three continuous variables : teachers, visualization

plt.hist(df['teachers'], color = 'yellow')
plt.title('histogram with number of teachers in average test score ')
plt.xlabel('teachers')
plt.ylabel('testscr')

# clean up data and set up 
y = df['testscr']
X = df.loc[:, ~df.columns.isin (['rownames','distcod','testscr', 'readscr', 'mathscr','district','grspan','county'])]

# drop all categorical variables and is not continuous
print(y[0:5])
X.head()

# set up training set and test set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)
X_train.head()

#KNN for regression

from sklearn.neighbors import KNeighborsRegressor
#train model with k = 5
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)

accuracy = knn.score(X_test, y_test)
print("Accuracy: {:.2f}".format(accuracy))

knn = KNeighborsRegressor(n_neighbors=7)
knn.fit(X_train, y_train)
#train model with k = 7

accuracy = knn.score(X_test, y_test)
print("Accuracy: {:.2f}".format(accuracy))

#import cross validation
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import RepeatedKFold
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import KFold

kfold = KFold(n_splits = 5)
rkf = RepeatedKFold(n_splits = 5, n_repeats = 10)

print("KFlod: "+str(cross_val_score(KNeighborsRegressor(n_neighbors=4), X_train, y_train, cv=kfold).mean()))
print("RepeatedKFold:\n{}".format(cross_val_score(KNeighborsRegressor(n_neighbors=5), X_train, y_train, cv=rkf).mean()))
print("KFlod: "+str(cross_val_score(KNeighborsRegressor(n_neighbors=4), X_train, y_train, cv=kfold)))

#OLS
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)

lr = LinearRegression()
lr.fit(X_train, y_train)

print(X_train.columns)  
print("lr.coef_: "+str(lr.coef_))  
print("lr.intercept_: {}".format(lr.intercept_))

from sklearn.model_selection import cross_val_score

print("Training set score: {:.2f}".format(lr.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lr.score(X_test, y_test)))

print(np.mean(cross_val_score(LinearRegression(), X_train, y_train, cv=kfold, scoring="r2")))

#no difference

import statsmodels.api as sm

X_train_new = sm.add_constant(X_train)
model = sm.OLS(y_train, X_train_new).fit()
model.summary()

# ridge regression

from sklearn.linear_model import Ridge
ridge = Ridge().fit(X_train, y_train)
print("1Training set score: {:.2f}".format(ridge.score(X_train, y_train)))
print("1Test set score: {:.2f}".format(ridge.score(X_test, y_test)))

# or (with default parameter)

ridge1 = Ridge(alpha=1).fit(X_train, y_train)
print("2Training set score: {:.2f}".format(ridge1.score(X_train, y_train)))
print("2Test set score: {:.2f}".format(ridge1.score(X_test, y_test)))

print(X_train.columns)  
print("ridge.coef_: {}".format(ridge.coef_))
print("ridge.intercept_: {}".format(ridge.intercept_))
#no difference

# Lasso regression

from sklearn.linear_model import Lasso
lasso = Lasso().fit(X_train, y_train)

print("Training set score: {:.2f}".format(lasso.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso.coef_ != 0)))

print(X_train.columns)  
print("lasso.coef_: {}".format(lasso.coef_))

# or 

lasso1 = Lasso(alpha=1, max_iter=100000).fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso1.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso1.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso.coef_ != 0)))

np.sum(lasso.coef_!=0)

# standard scaler, scaled data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# for knn regression 
#if k=5 in scaled data
knn = KNeighborsRegressor(n_neighbors=5)   
knn.fit(X_train_scaled, y_train)
accuracy = knn.score(X_test_scaled, y_test)
print("Accuracy:", accuracy)

# if k=3 in scaled data
knn = KNeighborsRegressor(n_neighbors=3)   
knn.fit(X_train_scaled, y_train)
accuracy = knn.score(X_test_scaled, y_test)
print("Accuracy:", accuracy)

# for linear regression

lr.fit(X_train_scaled, y_train)
train_score = lr.score(X_train_scaled, y_train)
test_score = lr.score(X_test_scaled, y_test)

print("Training set score: {:.2f}".format(train_score))
print("Test set score: {:.2f}".format(test_score))

# for ridge regression

ridge = Ridge().fit(X_train_scaled, y_train)

print("1Training set score: {:.2f}".format(ridge.score(X_train_scaled, y_train)))
print("1Test set score: {:.2f}".format(ridge.score(X_test_scaled, y_test)))

# for lasso regression

lasso = Lasso().fit(X_train_scaled, y_train)

print("Training set score: {:.2f}".format(lasso.score(X_train_scaled, y_train)))
print("Test set score: {:.2f}".format(lasso.score(X_test_scaled, y_test)))

# do cross validation, tune the parameters of the models

from sklearn.model_selection import GridSearchCV

#for knn regression
param_grid = {'n_neighbors': [1, 3, 5, 7, 9, 10, 11, 13]}
grid = GridSearchCV(KNeighborsRegressor(), param_grid = param_grid, cv=kfold)
grid.fit(X_train, y_train)

print("Best mean cross-validation score:{: 2f}".format(grid.best_score_))
print("Best parameters: {}".format(grid.best_params_))
print("test-set score: {:.2f}".format(grid.score(X_test, y_test)))

#model overfitting 

# for ridge

param_grid = {'alpha': [0.01, 1, 5, 10, 20, 50, 100]}
gridr = GridSearchCV(Ridge(), param_grid = param_grid, cv=kfold)
gridr.fit(X_train, y_train)

print("Best mean cross-validation score:{: 2f}".format(gridr.best_score_))
print("Best parameters: {}".format(gridr.best_params_))
print("test-set score: {:.2f}".format(gridr.score(X_test, y_test)))

#make more accurate prediction on new unseen data 

ridge10 = Ridge(alpha=10).fit(X_train, y_train)
print("3Training set score: {:.2f}".format(ridge10.score(X_train, y_train)))
print("3Test set score: {:.2f}".format(ridge10.score(X_test, y_test)))

ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)
print("4Training set score: {:.2f}".format(ridge01.score(X_train, y_train)))
print("4Test set score: {:.2f}".format(ridge01.score(X_test, y_test)))

# for lasso

param_grid = {'alpha': [0.01, 1, 5, 10, 20, 50, 100]}
gridl = GridSearchCV(Lasso(), param_grid = param_grid, cv=kfold)
gridl.fit(X_train, y_train)

print("Best mean cross-validation score:{: 2f}".format(gridl.best_score_))
print("Best parameters: {}".format(gridl.best_params_))
print("test-set score: {:.2f}".format(gridl.score(X_test, y_test)))

lasso001 = Lasso(alpha=0.001, max_iter=100000).fit(X_train, y_train)
print("Training set score: {:.2f}".format(lasso001.score(X_train, y_train)))
print("Test set score: {:.2f}".format(lasso001.score(X_test, y_test)))
print("Number of features used: {}".format(np.sum(lasso001.coef_ != 0)))
